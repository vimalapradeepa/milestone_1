# milestone_1
webscour_crawler_project 
Project Description â€“ Mini Web Crawler
This project implements a Mini Web Crawler using Python that automatically visits web pages starting from a given seed URL, downloads their content, extracts useful links, and continues crawling in a controlled manner. The crawler follows a Breadth-First Search (BFS) approach to explore web pages level by level while respecting defined limits.
The system is designed to:

Fetch web pages safely with retry and error handling
Extract only valid HTTP/HTTPS links
Avoid duplicate URL processing
Restrict crawling to the same domain
Save downloaded pages locally
Maintain a record of visited URLs
Generate a summary report showing performance metrics

Overall, this project demonstrates the core working principles of real-world web crawlers such as link discovery, duplicate prevention, domain control, and fault tolerance, making it suitable for academic learning and beginner-level web crawling applications.

